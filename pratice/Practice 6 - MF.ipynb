{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2018/19\n",
    "\n",
    "### Practice session on BPR-MF\n",
    "\n",
    "### Course survey on the challenge! https://goo.gl/forms/dgWszJeJW8D9I1Q62\n",
    "\n",
    "\n",
    "## Recap on BPR\n",
    "S.Rendle et al. BPR: Bayesian Personalized Ranking from Implicit Feedback. UAI2009\n",
    "\n",
    "The usual approach for item recommenders is to predict a personalized score $\\hat{x}_{ui}$ for an item that reflects the preference of the user for the item. Then the items are ranked by sorting them according to that score.\n",
    "\n",
    "Machine learning approaches are tipically fit by using observed items as a positive sample and missing ones for the negative class. A perfect model would thus be useless, as it would classify as negative (non-interesting) all the items that were non-observed at training time. The only reason why such methods work is regularization.\n",
    "\n",
    "BPR use a different approach. The training dataset is composed by triplets $(u,i,j)$ representing that user u is assumed to prefer i over j. For an implicit dataset this means that u observed i but not j:\n",
    "$$D_S := \\{(u,i,j) \\mid i \\in I_u^+ \\wedge j \\in I \\setminus I_u^+\\}$$\n",
    "\n",
    "### BPR-OPT\n",
    "A machine learning model can be represented by a parameter vector $\\Theta$ which is found at fitting time. BPR wants to find the parameter vector that is most probable given the desired, but latent, preference structure $>_u$:\n",
    "$$p(\\Theta \\mid >_u) \\propto p(>_u \\mid \\Theta)p(\\Theta) $$\n",
    "$$\\prod_{u\\in U} p(>_u \\mid \\Theta) = \\dots = \\prod_{(u,i,j) \\in D_S} p(i >_u j \\mid \\Theta) $$\n",
    "\n",
    "The probability that a user really prefers item $i$ to item $j$ is defined as:\n",
    "$$ p(i >_u j \\mid \\Theta) := \\sigma(\\hat{x}_{uij}(\\Theta)) $$\n",
    "Where $\\sigma$ represent the logistic sigmoid and $\\hat{x}_{uij}(\\Theta)$ is an arbitrary real-valued function of $\\Theta$ (the output of your arbitrary model).\n",
    "\n",
    "\n",
    "To complete the Bayesian setting, we define a prior density for the parameters:\n",
    "$$p(\\Theta) \\sim N(0, \\Sigma_\\Theta)$$\n",
    "And we can now formulate the maximum posterior estimator:\n",
    "$$BPR-OPT := \\log p(\\Theta \\mid >_u) $$\n",
    "$$ = \\log p(>_u \\mid \\Theta) p(\\Theta) $$\n",
    "$$ = \\log \\prod_{(u,i,j) \\in D_S} \\sigma(\\hat{x}_{uij})p(\\Theta) $$\n",
    "$$ = \\sum_{(u,i,j) \\in D_S} \\log \\sigma(\\hat{x}_{uij}) + \\log p(\\Theta) $$\n",
    "$$ = \\sum_{(u,i,j) \\in D_S} \\log \\sigma(\\hat{x}_{uij}) - \\lambda_\\Theta ||\\Theta||^2 $$\n",
    "\n",
    "Where $\\lambda_\\Theta$ are model specific regularization parameters.\n",
    "\n",
    "### BPR learning algorithm\n",
    "Once obtained the log-likelihood, we need to maximize it in order to find our obtimal $\\Theta$. As the crierion is differentiable, gradient descent algorithms are an obvious choiche for maximization.\n",
    "\n",
    "Gradient descent comes in many fashions, you can find an overview on my master thesis https://www.politesi.polimi.it/bitstream/10589/133864/3/tesi.pdf on pages 18-19-20 (I'm linking my thesis just because I'm sure of what it's written there, many posts you can find online contain some error). A nice post about momentum is available here https://distill.pub/2017/momentum/\n",
    "\n",
    "The basic version of gradient descent consists in evaluating the gradient using all the available samples and then perform a single update. The problem with this is, in our case, that our training dataset is very skewed. Suppose an item i is very popular. Then we habe many terms of the form $\\hat{x}_{uij}$ in the loss because for many users u the item i is compared against all negative items j.\n",
    "\n",
    "The other popular approach is stochastic gradient descent, where for each training sample an update is performed. This is a better approach, but the order in which the samples are traversed is crucial. To solve this issue BPR uses a stochastic gradient descent algorithm that choses the triples randomly.\n",
    "\n",
    "The gradient of BPR-OPT with respect to the model parameters is: \n",
    "$$\\frac{\\partial BPR-OPT}{\\partial \\Theta} = \\sum_{(u,i,j) \\in D_S} \\frac{\\partial}{\\partial \\Theta} \\log \\sigma (\\hat{x}_{uij}) - \\lambda_\\Theta \\frac{\\partial}{\\partial\\Theta} || \\Theta ||^2$$\n",
    "$$ =  \\sum_{(u,i,j) \\in D_S} \\frac{-e^{-\\hat{x}_{uij}}}{1+e^{-\\hat{x}_{uij}}} \\frac{\\partial}{\\partial \\Theta}\\hat{x}_{uij} - \\lambda_\\Theta \\Theta $$\n",
    "\n",
    "### BPR-MF\n",
    "\n",
    "In order to practically apply this learning schema to an existing algorithm, we first split the real valued preference term: $\\hat{x}_{uij} := \\hat{x}_{ui} − \\hat{x}_{uj}$. And now we can apply any standard collaborative filtering model that predicts $\\hat{x}_{ui}$.\n",
    "\n",
    "The problem of predicting $\\hat{x}_{ui}$ can be seen as the task of estimating a matrix $X:U×I$. With matrix factorization teh target matrix $X$ is approximated by the matrix product of two low-rank matrices $W:|U|\\times k$ and $H:|I|\\times k$:\n",
    "$$X := WH^t$$\n",
    "The prediction formula can also be written as:\n",
    "$$\\hat{x}_{ui} = \\langle w_u,h_i \\rangle = \\sum_{f=1}^k w_{uf} \\cdot h_{if}$$\n",
    "Besides the dot product ⟨⋅,⋅⟩, in general any kernel can be used.\n",
    "\n",
    "We can now specify the derivatives:\n",
    "$$ \\frac{\\partial}{\\partial \\theta} \\hat{x}_{uij} = \\begin{cases}\n",
    "(h_{if} - h_{jf}) \\text{ if } \\theta=w_{uf}, \\\\\n",
    "w_{uf} \\text{ if } \\theta = h_{if}, \\\\\n",
    "-w_{uf} \\text{ if } \\theta = h_{jf}, \\\\\n",
    "0 \\text{ else }\n",
    "\\end{cases} $$\n",
    "\n",
    "Which basically means: user $u$ prefer $i$ over $j$, let's do the following:\n",
    "- Increase the relevance (according to $u$) of features belonging to $i$ but not to $j$ and vice-versa\n",
    "- Increase the relevance of features assigned to $i$\n",
    "- Decrease the relevance of features assigned to $j$\n",
    "\n",
    "We're now ready to look at some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# skip the download\n",
    "#urlretrieve (\"http://files.grouplens.org/datasets/movielens/ml-10m.zip\", \"movielens_10m.zip\")\n",
    "dataFile = zipfile.ZipFile(\"movielens_10m.zip\")\n",
    "URM_path = dataFile.extract(\"ml-10M100K/ratings.dat\")\n",
    "URM_file = open(URM_path, 'r')\n",
    "\n",
    "\n",
    "def rowSplit (rowString):\n",
    "    \n",
    "    split = rowString.split(\"::\")\n",
    "    split[3] = split[3].replace(\"\\n\",\"\")\n",
    "    \n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = float(split[2])\n",
    "    split[3] = int(split[3])\n",
    "    \n",
    "    result = tuple(split)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "URM_file.seek(0)\n",
    "URM_tuples = []\n",
    "\n",
    "for line in URM_file:\n",
    "   URM_tuples.append(rowSplit (line))\n",
    "\n",
    "userList, itemList, ratingList, timestampList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "itemList = list(itemList)\n",
    "ratingList = list(ratingList)\n",
    "timestampList = list(timestampList)\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()\n",
    "\n",
    "\n",
    "\n",
    "from data_splitter import train_test_holdout\n",
    "\n",
    "\n",
    "URM_train, URM_test = train_test_holdout(URM_all, train_perc = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF Computing prediction\n",
    "\n",
    "### In a MF model you have two matrices, one with a row per user and the other with a column per item. The other dimension, columns for the first one and rows for the second one is called latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 10\n",
    "\n",
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "user_factors = np.random.random((n_users, num_factors))\n",
    "\n",
    "item_factors = np.random.random((n_items, num_factors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute the prediction we have to muliply the user factors to the item factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is 1.95\n"
     ]
    }
   ],
   "source": [
    "item_index = 15\n",
    "user_index = 42\n",
    "\n",
    "prediction = np.dot(user_factors[user_index,:], item_factors[item_index,:])\n",
    "\n",
    "print(\"Prediction is {:.2f}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MF MSE model\n",
    "\n",
    "### Use SGD as we saw for SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error is 3.05\n"
     ]
    }
   ],
   "source": [
    "test_data = 5\n",
    "learning_rate = 1e-2\n",
    "regularization = 1e-3\n",
    "\n",
    "gradient = test_data - prediction\n",
    "\n",
    "print(\"Prediction error is {:.2f}\".format(gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original value to avoid messing up the updates\n",
    "H_i = item_factors[item_index,:]\n",
    "W_u = user_factors[user_index,:]\n",
    "\n",
    "user_factors[user_index,:] += learning_rate * (gradient * H_i - regularization * W_u)\n",
    "item_factors[item_index,:] += learning_rate * (gradient * W_u - regularization * H_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after the update is 2.12\n",
      "Prediction error is 2.88\n"
     ]
    }
   ],
   "source": [
    "prediction = np.dot(user_factors[user_index,:], item_factors[item_index,:])\n",
    "\n",
    "print(\"Prediction after the update is {:.2f}\".format(prediction))\n",
    "print(\"Prediction error is {:.2f}\".format(test_data - prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING: Initialization must be done with random non-zero values ... otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors = np.zeros((n_users, num_factors))\n",
    "\n",
    "item_factors = np.zeros((n_items, num_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is 0.00\n",
      "Prediction error is 5.00\n"
     ]
    }
   ],
   "source": [
    "prediction = np.dot(user_factors[user_index,:], item_factors[item_index,:])\n",
    "\n",
    "print(\"Prediction is {:.2f}\".format(prediction))\n",
    "\n",
    "gradient = test_data - prediction\n",
    "\n",
    "print(\"Prediction error is {:.2f}\".format(gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_i = item_factors[item_index,:]\n",
    "W_u = user_factors[user_index,:]\n",
    "\n",
    "user_factors[user_index,:] += learning_rate * (gradient * H_i - regularization * W_u)\n",
    "item_factors[item_index,:] += learning_rate * (gradient * W_u - regularization * H_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after the update is 0.00\n",
      "Prediction error is 5.00\n"
     ]
    }
   ],
   "source": [
    "prediction = np.dot(user_factors[user_index,:], item_factors[item_index,:])\n",
    "\n",
    "print(\"Prediction after the update is {:.2f}\".format(prediction))\n",
    "print(\"Prediction error is {:.2f}\".format(test_data - prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the updates multiply the gradient and the latent factors, if those are zero the SGD will never be able to move from that point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MF BPR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics are the same, except for how we compute the gradient, we have to sample a triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_mask = URM_train.copy()\n",
    "URM_mask.data[URM_mask.data <= 3] = 0\n",
    "\n",
    "URM_mask.eliminate_zeros()\n",
    "\n",
    "# Extract users having at least one interaction to choose from\n",
    "eligibleUsers = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "\n",
    "    start_pos = URM_mask.indptr[user_id]\n",
    "    end_pos = URM_mask.indptr[user_id+1]\n",
    "\n",
    "    if len(URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "        eligibleUsers.append(user_id)\n",
    "                \n",
    "                \n",
    "\n",
    "def sampleTriplet():\n",
    "    \n",
    "    # By randomly selecting a user in this way we could end up \n",
    "    # with a user with no interactions\n",
    "    #user_id = np.random.randint(0, n_users)\n",
    "    \n",
    "    user_id = np.random.choice(eligibleUsers)\n",
    "    \n",
    "    # Get user seen items and choose one\n",
    "    userSeenItems = URM_mask[user_id,:].indices\n",
    "    pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "    negItemSelected = False\n",
    "\n",
    "    # It's faster to just try again then to build a mapping of the non-seen items\n",
    "    while (not negItemSelected):\n",
    "        neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "        if (neg_item_id not in userSeenItems):\n",
    "            \n",
    "            negItemSelected = True\n",
    "\n",
    "    return user_id, pos_item_id, neg_item_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62193, 798, 19585)\n",
      "(70514, 7, 26389)\n",
      "(45194, 3255, 61872)\n",
      "(67604, 1466, 2096)\n",
      "(69865, 3275, 24910)\n",
      "(36657, 281, 13051)\n",
      "(43716, 780, 53689)\n",
      "(55536, 4672, 1139)\n",
      "(68467, 6377, 8112)\n",
      "(47261, 1261, 31021)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampleTriplet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors = np.random.random((n_users, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52015 110 46652\n"
     ]
    }
   ],
   "source": [
    "user_id, positive_item, negative_item = sampleTriplet()\n",
    "\n",
    "print(user_id, positive_item, negative_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.369793948804664"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_uij = np.dot(user_factors[user_id, :], (item_factors[positive_item,:] - item_factors[negative_item,:]))\n",
    "\n",
    "x_uij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40859081173853357"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_item = 1 / (1 + np.exp(x_uij))\n",
    "\n",
    "sigmoid_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When using BPR we have to update three components, the user factors and the item factors of both the positive and negative item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "H_i = item_factors[positive_item,:]\n",
    "H_j = item_factors[negative_item,:]\n",
    "W_u = user_factors[user_id,:]\n",
    "\n",
    "\n",
    "user_factors[user_index,:] += learning_rate * (sigmoid_item * ( H_i - H_j ) - regularization * W_u)\n",
    "item_factors[positive_item,:] += learning_rate * (sigmoid_item * ( W_u ) - regularization * H_i)\n",
    "item_factors[negative_item,:] += learning_rate * (sigmoid_item * (-W_u ) - regularization * H_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4029981504158613"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_uij = np.dot(user_factors[user_id, :], (item_factors[positive_item,:] - item_factors[negative_item,:]))\n",
    "\n",
    "x_uij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.66240547, 2.68317025, 2.30127807, ..., 2.90799558, 2.91489822,\n",
       "       2.28287199])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## How to rank items with MF ?\n",
    "\n",
    "## Compute the prediction for all items and rank them\n",
    "\n",
    "item_scores = np.dot(user_factors[user_index,:], item_factors.T)\n",
    "item_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65134,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping, how to used and when it is needed\n",
    "\n",
    "### Problem, how many epochs? 5, 10, 150, 2487 ?\n",
    "\n",
    "### We could try different values in increasing order: 5, 10, 15, 20, 25...\n",
    "### However, in this way we would train up to a point, test and then discard the model, to re-train it again up to that same point and then some more... not a good idea.\n",
    "\n",
    "### Early stopping! \n",
    "* Train the model up to a certain number of epochs, say 5\n",
    "* Compute the recommendation quality on the validation set\n",
    "* Train for other 5 epochs\n",
    "* Compute the recommendation quality on the validation set AND compare it with the previous one. If better, then we have another best model, if not, go ahead...\n",
    "* Repeat until you have either reached the max number of epoch you want to allow (e.g., 300) or a certain number of contiguous validation seps have not updated te best model\n",
    "\n",
    "### Advantages:\n",
    "* Easy to implement, we already have all that is required, a train function, a predictor function and an evaluator\n",
    "* MUCH faster than retraining everything from the beginning\n",
    "* Often allows to reach even better solutions\n",
    "\n",
    "### Challenges:\n",
    "* The evaluation step may be very slow compared to the time it takes to re-train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a PureSVD model\n",
    "\n",
    "### As opposed to the previous ones, PureSVD relies on the SVD decomposition of the URM, which is an easily available function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# Other SVDs are also available, like from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, VT = randomized_svd(URM_train,\n",
    "              n_components=num_factors,\n",
    "              #n_iter=5,\n",
    "              random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.15237057e-22,  9.00042161e-17, -6.15534479e-17, ...,\n",
       "         1.87329476e-15, -4.80164702e-16, -4.26346115e-15],\n",
       "       [ 1.21492820e-03, -5.16737846e-03, -1.24882608e-03, ...,\n",
       "        -7.11839056e-04, -3.37269665e-03, -4.72611719e-04],\n",
       "       [ 4.83249603e-04, -1.09828743e-03, -2.49772391e-04, ...,\n",
       "         1.76756648e-03,  2.44699593e-03,  2.16138711e-03],\n",
       "       ...,\n",
       "       [ 3.70060108e-03,  9.56709754e-04,  5.88503411e-03, ...,\n",
       "        -2.04610436e-03,  1.31130620e-04,  1.02005676e-03],\n",
       "       [ 1.33390185e-03, -5.37372173e-03,  1.34196826e-03, ...,\n",
       "        -6.12238053e-05, -3.61423117e-03,  3.51365584e-04],\n",
       "       [ 1.17462706e-03, -7.28843615e-05, -6.36426083e-04, ...,\n",
       "         2.65369987e-03,  2.22794340e-04,  3.64775621e-03]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71568, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2680.11168452, 1133.17197747,  968.50733478,  788.3592598 ,\n",
       "        765.26720266,  658.77648479,  625.37116039,  601.91077022,\n",
       "        554.41040645,  495.42954463])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.01998101e-22,  8.09465011e-02,  3.46101683e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.18048448e-05],\n",
       "       [-5.24425744e-16, -4.45149520e-02, -4.77749172e-02, ...,\n",
       "        -0.00000000e+00, -0.00000000e+00,  6.34296578e-05],\n",
       "       [ 1.78822352e-16, -1.10194013e-02, -2.07431054e-02, ...,\n",
       "        -0.00000000e+00, -0.00000000e+00, -9.03552118e-06],\n",
       "       ...,\n",
       "       [ 9.07939885e-17,  1.64175618e-01,  2.98834918e-02, ...,\n",
       "        -0.00000000e+00, -0.00000000e+00,  1.93950322e-04],\n",
       "       [-7.33733439e-17, -5.08572753e-02, -2.77533977e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  5.37135109e-06],\n",
       "       [ 5.76734943e-17,  7.17175321e-03, -2.74041953e-03, ...,\n",
       "        -0.00000000e+00, -0.00000000e+00, -7.20659786e-05]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 65134)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncating the number of singular values introduces an approximation which allows to fill the missing urm entries\n",
    "\n",
    "### Computing a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store an intermediate pre-multiplied matrix\n",
    "\n",
    "s_Vt = sps.diags(Sigma)*VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is 0.02\n"
     ]
    }
   ],
   "source": [
    "prediction = U[user_index, :].dot(s_Vt[:,item_index])\n",
    "\n",
    "print(\"Prediction is {:.2f}\".format(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.52848960e-16, 3.78808935e-01, 2.20850783e-01, ...,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.68948795e-04])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_scores = U[user_index, :].dot(s_Vt)\n",
    "item_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65134,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's compare the three MF: BPR, FunkSVD and PureSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixFactorization.Cython.MatrixFactorization_Cython import MatrixFactorization_BPR_Cython, MatrixFactorization_FunkSVD_Cython\n",
    "from MatrixFactorization.PureSVD import PureSVDRecommender\n",
    "\n",
    "from Base.Evaluation.Evaluator import SequentialEvaluator\n",
    "\n",
    "evaluator_test = SequentialEvaluator(URM_test, cutoff_list=[5])\n",
    "\n",
    "evaluator_validation_early_stopping = SequentialEvaluator(URM_train, cutoff_list=[5], exclude_seen = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 71568 ( 100.00% ) in 0.99 seconds. BPR loss 8.11E-02. Sample per second: 72126\n",
      "MF_BPR: Epoch 1 of 300. Elapsed time 0.00 min\n",
      "Processed 71568 ( 100.00% ) in 1.06 seconds. BPR loss 1.96E-01. Sample per second: 67518\n",
      "MF_BPR: Epoch 2 of 300. Elapsed time 0.00 min\n",
      "Processed 71568 ( 100.00% ) in 0.14 seconds. BPR loss 3.53E-01. Sample per second: 515923\n",
      "MF_BPR: Epoch 3 of 300. Elapsed time 0.00 min\n",
      "Processed 71568 ( 100.00% ) in 0.20 seconds. BPR loss 5.32E-01. Sample per second: 353564\n",
      "MF_BPR: Epoch 4 of 300. Elapsed time 0.00 min\n",
      "Processed 71568 ( 100.00% ) in 0.27 seconds. BPR loss 7.42E-01. Sample per second: 260826\n",
      "MF_BPR: Epoch 5 of 300. Elapsed time 0.01 min\n",
      "Processed 71568 ( 100.00% ) in 0.37 seconds. BPR loss 9.36E-01. Sample per second: 193566\n",
      "MF_BPR: Epoch 6 of 300. Elapsed time 0.01 min\n",
      "Processed 71568 ( 100.00% ) in 0.46 seconds. BPR loss 1.13E+00. Sample per second: 155037\n",
      "MF_BPR: Epoch 7 of 300. Elapsed time 0.01 min\n",
      "Processed 71568 ( 100.00% ) in 0.52 seconds. BPR loss 1.36E+00. Sample per second: 137605\n",
      "MF_BPR: Epoch 8 of 300. Elapsed time 0.01 min\n",
      "Processed 71568 ( 100.00% ) in 0.58 seconds. BPR loss 1.56E+00. Sample per second: 122480\n",
      "MF_BPR: Epoch 9 of 300. Elapsed time 0.01 min\n",
      "Processed 71568 ( 100.00% ) in 0.68 seconds. BPR loss 1.77E+00. Sample per second: 105850\n",
      "MF_BPR: Validation begins...\n",
      "SequentialEvaluator: Processed 22001 ( 31.48% ) in 30.40 seconds. Users per second: 724\n",
      "SequentialEvaluator: Processed 45001 ( 64.40% ) in 61.03 seconds. Users per second: 737\n",
      "SequentialEvaluator: Processed 68001 ( 97.31% ) in 91.55 seconds. Users per second: 743\n",
      "SequentialEvaluator: Processed 69878 ( 100.00% ) in 92.99 seconds. Users per second: 751\n",
      "MF_BPR: {'ROC_AUC': 0.3163096158065981, 'PRECISION': 0.23365723117437867, 'RECALL': 0.03054872061174214, 'RECALL_TEST_LEN': 0.23365723117437867, 'MAP': 0.1526032513810113, 'MRR': 0.38152065027618737, 'NDCG': 0.06719717735776935, 'F1': 0.054033070987269914, 'HIT_RATE': 1.1682503792323764, 'ARHR': 0.5430075751070585, 'NOVELTY': 0.0006341791559732849, 'DIVERSITY_MEAN_INTER_LIST': 0.0, 'DIVERSITY_HERFINDAHL': 0.7999999999999999, 'COVERAGE_ITEM': 7.676482328737679e-05, 'COVERAGE_USER': 0.9763860943438408, 'DIVERSITY_GINI': 1.0, 'SHANNON_ENTROPY': 2.321928094887362}\n",
      "MF_BPR: Epoch 10 of 300. Elapsed time 1.56 min\n",
      "Processed 71568 ( 100.00% ) in 0.75 seconds. BPR loss 2.01E+00. Sample per second: 95480\n",
      "MF_BPR: Epoch 11 of 300. Elapsed time 1.56 min\n",
      "Processed 71568 ( 100.00% ) in 0.81 seconds. BPR loss 2.20E+00. Sample per second: 88297\n",
      "MF_BPR: Epoch 12 of 300. Elapsed time 1.56 min\n",
      "Processed 71568 ( 100.00% ) in 0.87 seconds. BPR loss 2.44E+00. Sample per second: 82205\n",
      "MF_BPR: Epoch 13 of 300. Elapsed time 1.57 min\n",
      "Processed 71568 ( 100.00% ) in 0.93 seconds. BPR loss 2.66E+00. Sample per second: 76966\n",
      "MF_BPR: Epoch 14 of 300. Elapsed time 1.57 min\n",
      "Processed 71568 ( 100.00% ) in 0.99 seconds. BPR loss 2.90E+00. Sample per second: 72399\n",
      "MF_BPR: Epoch 15 of 300. Elapsed time 1.57 min\n",
      "Processed 71568 ( 100.00% ) in 1.05 seconds. BPR loss 3.09E+00. Sample per second: 68370\n",
      "MF_BPR: Epoch 16 of 300. Elapsed time 1.57 min\n",
      "Processed 71568 ( 100.00% ) in 0.11 seconds. BPR loss 3.31E+00. Sample per second: 679448\n",
      "MF_BPR: Epoch 17 of 300. Elapsed time 1.57 min\n",
      "Processed 71568 ( 100.00% ) in 0.16 seconds. BPR loss 3.52E+00. Sample per second: 436743\n",
      "MF_BPR: Epoch 18 of 300. Elapsed time 1.57 min\n",
      "Processed 71568 ( 100.00% ) in 0.22 seconds. BPR loss 3.74E+00. Sample per second: 321309\n",
      "MF_BPR: Epoch 19 of 300. Elapsed time 1.57 min\n",
      "Processed 71568 ( 100.00% ) in 0.28 seconds. BPR loss 4.02E+00. Sample per second: 254305\n",
      "MF_BPR: Validation begins...\n",
      "SequentialEvaluator: Processed 21001 ( 30.05% ) in 30.26 seconds. Users per second: 694\n",
      "SequentialEvaluator: Processed 43001 ( 61.54% ) in 60.67 seconds. Users per second: 709\n",
      "SequentialEvaluator: Processed 65273 ( 93.41% ) in 90.67 seconds. Users per second: 720\n",
      "SequentialEvaluator: Processed 69878 ( 100.00% ) in 96.22 seconds. Users per second: 726\n",
      "MF_BPR: {'ROC_AUC': 0.3211680834215851, 'PRECISION': 0.23365723117437867, 'RECALL': 0.03054872061174214, 'RECALL_TEST_LEN': 0.23365723117437867, 'MAP': 0.15299381302654252, 'MRR': 0.382171785111181, 'NDCG': 0.06730414235368906, 'F1': 0.054033070987269914, 'HIT_RATE': 1.1682503792323764, 'ARHR': 0.5442335212799118, 'NOVELTY': 0.0006341791559732849, 'DIVERSITY_MEAN_INTER_LIST': 0.0, 'DIVERSITY_HERFINDAHL': 0.7999999999999999, 'COVERAGE_ITEM': 7.676482328737679e-05, 'COVERAGE_USER': 0.9763860943438408, 'DIVERSITY_GINI': 1.0, 'SHANNON_ENTROPY': 2.321928094887362}\n",
      "MF_BPR: Epoch 20 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 0.58 seconds. BPR loss 4.23E+00. Sample per second: 122642\n",
      "MF_BPR: Epoch 21 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 0.66 seconds. BPR loss 4.50E+00. Sample per second: 108652\n",
      "MF_BPR: Epoch 22 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 0.73 seconds. BPR loss 4.60E+00. Sample per second: 97796\n",
      "MF_BPR: Epoch 23 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 0.80 seconds. BPR loss 4.93E+00. Sample per second: 89769\n",
      "MF_BPR: Epoch 24 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 0.86 seconds. BPR loss 5.09E+00. Sample per second: 83409\n",
      "MF_BPR: Epoch 25 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 0.92 seconds. BPR loss 5.32E+00. Sample per second: 77948\n",
      "MF_BPR: Epoch 26 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 0.98 seconds. BPR loss 5.53E+00. Sample per second: 73231\n",
      "MF_BPR: Epoch 27 of 300. Elapsed time 3.18 min\n",
      "Processed 71568 ( 100.00% ) in 1.04 seconds. BPR loss 5.67E+00. Sample per second: 69085\n",
      "MF_BPR: Epoch 28 of 300. Elapsed time 3.19 min\n",
      "Processed 71568 ( 100.00% ) in 0.10 seconds. BPR loss 5.95E+00. Sample per second: 718867\n",
      "MF_BPR: Epoch 29 of 300. Elapsed time 3.19 min\n",
      "Processed 71568 ( 100.00% ) in 0.16 seconds. BPR loss 6.16E+00. Sample per second: 446103\n",
      "MF_BPR: Validation begins...\n",
      "SequentialEvaluator: Processed 21001 ( 30.05% ) in 30.35 seconds. Users per second: 692\n",
      "SequentialEvaluator: Processed 43001 ( 61.54% ) in 60.76 seconds. Users per second: 708\n",
      "SequentialEvaluator: Processed 64001 ( 91.59% ) in 91.50 seconds. Users per second: 699\n",
      "SequentialEvaluator: Processed 69878 ( 100.00% ) in 99.26 seconds. Users per second: 704\n",
      "MF_BPR: {'ROC_AUC': 0.3234518255626493, 'PRECISION': 0.23365723117437867, 'RECALL': 0.03054872061174214, 'RECALL_TEST_LEN': 0.23365723117437867, 'MAP': 0.15331990516808397, 'MRR': 0.38285678849803545, 'NDCG': 0.06733725933733757, 'F1': 0.054033070987269914, 'HIT_RATE': 1.1682503792323764, 'ARHR': 0.5453747960731241, 'NOVELTY': 0.0006341791559732849, 'DIVERSITY_MEAN_INTER_LIST': 0.0, 'DIVERSITY_HERFINDAHL': 0.7999999999999999, 'COVERAGE_ITEM': 7.676482328737679e-05, 'COVERAGE_USER': 0.9763860943438408, 'DIVERSITY_GINI': 1.0, 'SHANNON_ENTROPY': 2.321928094887362}\n",
      "MF_BPR: Epoch 30 of 300. Elapsed time 4.84 min\n",
      "Processed 71568 ( 100.00% ) in 0.51 seconds. BPR loss 6.35E+00. Sample per second: 140296\n",
      "MF_BPR: Epoch 31 of 300. Elapsed time 4.84 min\n",
      "Processed 71568 ( 100.00% ) in 0.58 seconds. BPR loss 6.58E+00. Sample per second: 123202\n",
      "MF_BPR: Epoch 32 of 300. Elapsed time 4.84 min\n",
      "Processed 71568 ( 100.00% ) in 0.65 seconds. BPR loss 6.75E+00. Sample per second: 109352\n",
      "MF_BPR: Epoch 33 of 300. Elapsed time 4.85 min\n",
      "Processed 71568 ( 100.00% ) in 0.72 seconds. BPR loss 6.98E+00. Sample per second: 99029\n",
      "MF_BPR: Epoch 34 of 300. Elapsed time 4.85 min\n",
      "Processed 71568 ( 100.00% ) in 0.79 seconds. BPR loss 7.18E+00. Sample per second: 90733\n",
      "MF_BPR: Epoch 35 of 300. Elapsed time 4.85 min\n",
      "Processed 71568 ( 100.00% ) in 0.85 seconds. BPR loss 7.38E+00. Sample per second: 83854\n",
      "MF_BPR: Epoch 36 of 300. Elapsed time 4.85 min\n",
      "Processed 71568 ( 100.00% ) in 0.92 seconds. BPR loss 7.62E+00. Sample per second: 77771\n",
      "MF_BPR: Epoch 37 of 300. Elapsed time 4.85 min\n",
      "Processed 71568 ( 100.00% ) in 0.99 seconds. BPR loss 7.77E+00. Sample per second: 72038\n",
      "MF_BPR: Epoch 38 of 300. Elapsed time 4.85 min\n",
      "Processed 71568 ( 100.00% ) in 1.06 seconds. BPR loss 7.94E+00. Sample per second: 67339\n",
      "MF_BPR: Epoch 39 of 300. Elapsed time 4.85 min\n",
      "Processed 71568 ( 100.00% ) in 0.13 seconds. BPR loss 8.17E+00. Sample per second: 535772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF_BPR: Validation begins...\n",
      "SequentialEvaluator: Processed 20996 ( 30.05% ) in 30.00 seconds. Users per second: 700\n",
      "SequentialEvaluator: Processed 42001 ( 60.11% ) in 60.08 seconds. Users per second: 699\n"
     ]
    }
   ],
   "source": [
    "recommender = MatrixFactorization_BPR_Cython(URM_train)\n",
    "recommender.fit(num_factors = 50, \n",
    "                validation_every_n = 10, \n",
    "                stop_on_validation = True, \n",
    "                evaluator_object = evaluator_validation_early_stopping,\n",
    "                lower_validatons_allowed = 5, \n",
    "                validation_metric = \"MAP\")\n",
    "\n",
    "result_dict, _ = evaluator_test.evaluateRecommender(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = MatrixFactorization_FunkSVD_Cython(URM_train)\n",
    "recommender.fit(num_factors = 50, \n",
    "                validation_every_n = 10, \n",
    "                stop_on_validation = True, \n",
    "                evaluator_object = evaluator_validation_early_stopping,\n",
    "                lower_validatons_allowed = 5, \n",
    "                validation_metric = \"MAP\")\n",
    "\n",
    "result_dict, _ = evaluator_test.evaluateRecommender(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = PureSVDRecommender(URM_train)\n",
    "recommender.fit()\n",
    "\n",
    "result_dict, _ = evaluator_test.evaluateRecommender(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
