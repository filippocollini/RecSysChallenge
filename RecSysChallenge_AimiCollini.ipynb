{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeSjz9AcKr73"
   },
   "source": [
    "<h1>Recommender system challenge PoliMi 2018</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import dependencies</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import recommender as recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>See if we upload it correctly</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204.0
    },
    "colab_type": "code",
    "id": "GN7X-oe9SHoc",
    "outputId": "fcf2f723-6cd5-44ba-93d3-0c6edf2612b2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>18397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playlist_id  track_id\n",
       "0            0     14301\n",
       "1            0      8360\n",
       "2            0     12844\n",
       "3            0     18397\n",
       "4            0      1220"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"all/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlist_list = list(np.asarray(train['playlist_id']))\n",
    "track_list = list(np.asarray(train['track_id']))\n",
    "playlist_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204.0
    },
    "colab_type": "code",
    "id": "IBbrfdWjTCUF",
    "outputId": "b5578f07-6d8e-4bb6-c59d-edd830635a99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>album_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6306</td>\n",
       "      <td>449</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12085</td>\n",
       "      <td>4903</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1885</td>\n",
       "      <td>6358</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3989</td>\n",
       "      <td>1150</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11633</td>\n",
       "      <td>4447</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id  album_id  artist_id  duration_sec\n",
       "0         0      6306        449           167\n",
       "1         1     12085       4903           185\n",
       "2         2      1885       6358           201\n",
       "3         3      3989       1150           263\n",
       "4         4     11633       4447            96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = pd.read_csv(\"all/tracks.csv\")\n",
    "tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playlist_id\n",
       "0            7\n",
       "1           25\n",
       "2           29\n",
       "3           34\n",
       "4           50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"all/target_playlists.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_playlist_list = list(np.asarray(test['playlist_id']))\n",
    "all_track_list = list(np.asarray(train['track_id']))\n",
    "album_list = list(np.asarray(tracks['album_id']))\n",
    "artist_list = list(np.asarray(tracks['artist_id']))\n",
    "duration_list = list(np.asarray(tracks['duration_sec']))\n",
    "all_tracks_in_tracks_list = list(np.asarray(tracks['track_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211791"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of different playlist, tracks, albums, artists\n",
    "playlist_unique = list(set(playlist_list))\n",
    "track_unique = list(set(track_list))\n",
    "album_unique = list(set(album_list))\n",
    "artist_unique = list(set(artist_list))\n",
    "\n",
    "num_playlists = len(playlist_list)\n",
    "num_tracks = len(track_list)\n",
    "num_albums = len(album_unique)\n",
    "num_artists = len(artist_unique)\n",
    "\n",
    "num_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ones((num_playlists), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50446x20635 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1211791 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sps\n",
    "                         \n",
    "URM_train = sps.coo_matrix((data, (playlist_list, track_list)))\n",
    "\n",
    "URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50446x20635 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1211791 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211791"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split = 0.80\n",
    "\n",
    "numInteractions = URM_train.getnnz()\n",
    "numInteractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask = np.random.choice([True,False], numInteractions, p=[train_test_split, 1-train_test_split])\n",
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50446x20635 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 969579 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_list = np.array(track_list)\n",
    "playlist_list = np.array(playlist_list)\n",
    "all_track_list = np.array(all_track_list)\n",
    "#train_dummies = np.array(train_dummies)\n",
    "\n",
    "#len(train_dummies)\n",
    "#len(train_mask)\n",
    "\n",
    "URM_train = sps.coo_matrix((data[train_mask], (playlist_list[train_mask], track_list[train_mask])))\n",
    "URM_train = URM_train.tocsr()\n",
    "URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50446x20635 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 242212 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask = np.logical_not(train_mask)\n",
    "\n",
    "URM_test = sps.coo_matrix((data[test_mask], (playlist_list[test_mask], track_list[test_mask])))\n",
    "URM_test = URM_test.tocsr()\n",
    "URM_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Mean Average Precision</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm_different(URM_test, recommender_object, at=10):\n",
    "    \n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "    for playlist_id in playlist_unique:\n",
    "\n",
    "        relevant_items = URM_test[playlist_id].indices\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            #recommended_items = recommender_object.recommend(playlist_id, at=at)\n",
    "            recommended_items = recommender_object.recommend(playlist_id)\n",
    "            num_eval+=1\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: MAP = {:.4f}\".format(cumulative_MAP)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(URM_test, recommender_object, at=10):\n",
    "    \n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "    for playlist_id in playlist_unique:\n",
    "\n",
    "        relevant_items = URM_test[playlist_id].indices\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            recommended_items = recommender_object.recommend(playlist_id, at=at)\n",
    "            num_eval+=1\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: MAP = {:.4f}\".format(cumulative_MAP)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TopPop Recommender</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopPopRecommender(object):\n",
    "\n",
    "    def fit(self, URM_train):\n",
    "    \n",
    "        self.URM_train = URM_train\n",
    "    \n",
    "        itemPopularity = (URM_train>0).sum(axis=0)\n",
    "        itemPopularity = np.array(itemPopularity).squeeze()\n",
    "\n",
    "        # We are not interested in sorting the popularity value,\n",
    "        # but to order the items according to it\n",
    "        self.popularItems = np.argsort(itemPopularity)\n",
    "        self.popularItems = np.flip(self.popularItems, axis = 0)\n",
    "    \n",
    "    \n",
    "    def recommend(self, playlist_id, at=10, remove_seen=True):\n",
    "        \n",
    "        if remove_seen:\n",
    "            unseen_items_mask = np.in1d(self.popularItems, \n",
    "                                        self.URM_train[playlist_id].indices, \n",
    "                                        assume_unique = True, \n",
    "                                        invert = True)\n",
    "            \n",
    "            unseen_items = self.popularItems[unseen_items_mask]\n",
    "\n",
    "            recommended_items = unseen_items[0:at]\n",
    "        \n",
    "        else:\n",
    "            recommended_items = self.popularItems[0:at]\n",
    "\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fit and test the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "topPopRecommender = TopPopRecommender()\n",
    "topPopRecommender.fit(URM_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n",
      "[ 8956 10848  5606 15578 10496  2674 13980 17239 18266  2272]\n"
     ]
    }
   ],
   "source": [
    "for playlist_id in playlist_unique[0:10]:\n",
    "    print(topPopRecommender.recommend(playlist_id, at=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender performance is: MAP = 0.0043\n"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm(URM_test, topPopRecommender, at=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Works!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code for creation of the submission csv</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_playlist_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=[\"playlist_id\",\"track_ids\"])\n",
    "\n",
    "for playlist_id in test_playlist_list: \n",
    "    recommendation = ' '.join(map(str, topPopRecommender.recommend(playlist_id, at=10)))\n",
    "    row = pd.DataFrame([[playlist_id,recommendation]], columns=[\"playlist_id\",\"track_ids\"])\n",
    "    submission = submission.append(row)\n",
    "\n",
    "submission.to_csv(\"all/sub.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Content based similarity</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20635x6668 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20635 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = np.ones((num_tracks_in_tracks), dtype=int)\n",
    "\n",
    "ICM_all_artist = sps.coo_matrix((ones, (tracks_in_tracks, artist_list)))\n",
    "ICM_all_artist = ICM_all_artist.tocsr()\n",
    "\n",
    "ICM_all_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20635x12744 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20635 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = np.ones((num_tracks_in_tracks), dtype=int)\n",
    "\n",
    "ICM_all_album = sps.coo_matrix((ones, (tracks_in_tracks, album_list)))\n",
    "ICM_all_album = ICM_all_album.tocsr()\n",
    "\n",
    "ICM_all_album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20635x2115 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20635 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = np.ones((num_tracks_in_tracks), dtype=int)\n",
    "\n",
    "ICM_all_duration = sps.coo_matrix((ones, (tracks_in_tracks, duration_list)))\n",
    "ICM_all_duration = ICM_all_duration.tocsr()\n",
    "\n",
    "ICM_all_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicItemKNNRecommender(object):\n",
    "    \"\"\" ItemKNN recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM, k=50, shrinkage=100, similarity='cosine'):\n",
    "        self.dataset = URM\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'pearson':\n",
    "            self.distance = Pearson(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'adj-cosine':\n",
    "            self.distance = AdjustedCosine(shrinkage=self.shrinkage)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ItemKNN(similarity={},k={},shrinkage={})\".format(\n",
    "            self.similarity_name, self.k, self.shrinkage)\n",
    "\n",
    "    def fit(self, X):\n",
    "        item_weights = self.distance.compute(X)\n",
    "        \n",
    "        item_weights = check_matrix(item_weights, 'csr') # nearly 10 times faster\n",
    "        print(\"Converted to csr\")\n",
    "        \n",
    "        # for each column, keep only the top-k scored items\n",
    "        # THIS IS THE SLOW PART, FIND A BETTER SOLUTION        \n",
    "        values, rows, cols = [], [], []\n",
    "        nitems = self.dataset.shape[1]\n",
    "        for i in range(nitems):\n",
    "            if (i % 10000 == 0):\n",
    "                print(\"Item %d of %d\" % (i, nitems))\n",
    "                \n",
    "            this_item_weights = item_weights[i,:].toarray()[0]\n",
    "            top_k_idx = np.argsort(this_item_weights) [-self.k:]\n",
    "                        \n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(nitems)[top_k_idx])\n",
    "            cols.extend(np.ones(self.k) * i)\n",
    "        self.W_sparse = sps.csc_matrix((values, (rows, cols)), shape=(nitems, nitems), dtype=np.float32)\n",
    "\n",
    "    def recommend(self, playlist_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.dataset[playlist_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(playlist_id, ranking)\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    def _filter_seen(self, playlist_id, ranking):\n",
    "        user_profile = self.dataset[playlist_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "class ISimilarity(object):\n",
    "    \"\"\"Abstract interface for the similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, shrinkage=10):\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def compute(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cosine(ISimilarity):\n",
    "    def compute(self, X):\n",
    "        # convert to csc matrix for faster column-wise operations\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "\n",
    "        # 1) normalize the columns in X\n",
    "        # compute the column-wise norm\n",
    "        # NOTE: this is slightly inefficient. We must copy X to compute the column norms.\n",
    "        # A faster solution is to  normalize the matrix inplace with a Cython function.\n",
    "        Xsq = X.copy()\n",
    "        Xsq.data **= 2\n",
    "        norm = np.sqrt(Xsq.sum(axis=0))\n",
    "        norm = np.asarray(norm).ravel()\n",
    "        norm += 1e-6\n",
    "        # compute the number of non-zeros in each column\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(X.indptr)\n",
    "        # then normalize the values in each column\n",
    "        X.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "\n",
    "        # 2) compute the cosine similarity using the dot-product\n",
    "        dist = X * X.T\n",
    "        print(\"Computed\")\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        dist = dist - sps.dia_matrix((dist.diagonal()[scipy.newaxis, :], [0]), shape=dist.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            dist = self.apply_shrinkage(X, dist)\n",
    "            print(\"Applied shrinkage\")    \n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test it</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Artists</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Converted to csr\n",
      "Item 0 of 20635\n",
      "Item 10000 of 20635\n",
      "Item 20000 of 20635\n"
     ]
    }
   ],
   "source": [
    "rec = BasicItemKNNRecommender(URM=URM_train, shrinkage=0.0, k=50)\n",
    "rec.fit(ICM_all_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6095 19582 10912 19034 18427  2355 14796  5957  1568  2506]\n",
      "[11605  8907 16515 17519 14741 11861 18466  3896  8727 12388]\n",
      "[15130 15679 11228 10472 15506 13364  3151  7112 12460  8796]\n",
      "[ 5218  9348 13503  4379 11327  9740  3292 17348 19475  3321]\n",
      "[ 9576  3493 10927 13052  2528 13812  3747 12601 10023 11958]\n",
      "[  243 19620  8804  6879  6874  6875  6876  6877  6878  6886]\n",
      "[ 4103  7906 13979   102  9550  2385  6954 10945  6725  7803]\n",
      "[ 4542  9570  4032  3052  1651 11776  1790 15908  4492 15124]\n",
      "[ 2672 14049 18630 15614  5179  6460 10585  1025 11251 17148]\n",
      "[ 3523  2230 18769  7031 11277  7646 14730  3200 20513  3659]\n"
     ]
    }
   ],
   "source": [
    "for playlist_id in playlist_unique[0:10]:\n",
    "    print(rec.recommend(playlist_id, at=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender performance is: MAP = 0.0317\n"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm(URM_test, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Applied shrinkage\n",
      "Converted to csr\n",
      "Item 0 of 20635\n",
      "Item 10000 of 20635\n",
      "Item 20000 of 20635\n",
      "Recommender performance is: MAP = 0.0317\n"
     ]
    }
   ],
   "source": [
    "rec_s_artist = BasicItemKNNRecommender(URM=URM_train, shrinkage=10.0, k=50)\n",
    "rec_s_artist.fit(ICM_all_artist)\n",
    "evaluate_algorithm(URM_test, rec_s_artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Albums</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Applied shrinkage\n",
      "Converted to csr\n",
      "Item 0 of 20635\n",
      "Item 10000 of 20635\n",
      "Item 20000 of 20635\n",
      "Recommender performance is: MAP = 0.0499\n"
     ]
    }
   ],
   "source": [
    "rec_s_album = BasicItemKNNRecommender(URM=URM_train, shrinkage=10.0, k=50)\n",
    "rec_s_album.fit(ICM_all_album)\n",
    "evaluate_algorithm(URM_test, rec_s_album)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Durations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Applied shrinkage\n",
      "Converted to csr\n",
      "Item 0 of 20635\n",
      "Item 10000 of 20635\n",
      "Item 20000 of 20635\n",
      "Recommender performance is: MAP = 0.0001\n"
     ]
    }
   ],
   "source": [
    "rec_s_duration = BasicItemKNNRecommender(URM=URM_train, shrinkage=10.0, k=50)\n",
    "rec_s_duration.fit(ICM_all_duration)\n",
    "evaluate_algorithm(URM_test, rec_s_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's try to add collaborative filtering</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.sparse.linalg import (inv, spsolve)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class collaborative_filtering(object):\n",
    "    \n",
    "    def nonzeros(self, m, row):\n",
    "        for index in range(m.indptr[row], m.indptr[row+1]):\n",
    "            yield m.indices[index], m.data[index]\n",
    "            \n",
    "    def least_squares_cg(self, Cui, X, Y, lambda_val, cg_steps=3):\n",
    "        users, features = X.shape\n",
    "\n",
    "        YtY = Y.T.dot(Y) + lambda_val * np.eye(features)\n",
    "\n",
    "        for u in range(users):\n",
    "\n",
    "            x = X[u]\n",
    "            r = -YtY.dot(x)\n",
    "\n",
    "            for i, confidence in self.nonzeros(Cui, u):\n",
    "                r += (confidence - (confidence - 1) * Y[i].dot(x)) * Y[i]\n",
    "\n",
    "            p = r.copy()\n",
    "            rsold = r.dot(r)\n",
    "\n",
    "            for it in range(cg_steps):\n",
    "                Ap = YtY.dot(p)\n",
    "                for i, confidence in self.nonzeros(Cui, u):\n",
    "                    Ap += (confidence - 1) * Y[i].dot(p) * Y[i]\n",
    "\n",
    "                alpha = rsold / p.dot(Ap)\n",
    "                x += alpha * p\n",
    "                r -= alpha * Ap\n",
    "\n",
    "                rsnew = r.dot(r)\n",
    "                p = r + (rsnew / rsold) * p\n",
    "                rsold = rsnew\n",
    "\n",
    "            X[u] = x\n",
    "            \n",
    "    def recommend(self, playlist_id, at=10):\n",
    "        \n",
    "        # Get all interactions by the user\n",
    "        user_interactions = self.Cui[playlist_id,:].toarray()\n",
    "\n",
    "        # We don't want to recommend items the user has consumed. So let's\n",
    "        # set them all to 0 and the unknowns to 1.\n",
    "        user_interactions = user_interactions.reshape(-1) + 1 #Reshape to turn into 1D array\n",
    "        user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "        # This is where we calculate the recommendation by taking the \n",
    "        # dot-product of the user vectors with the item vectors.\n",
    "        rec_vector = self.X_sparse[playlist_id,:].dot(self.Y_sparse.T).toarray()\n",
    "\n",
    "        # Let's scale our scores between 0 and 1 to make it all easier to interpret.\n",
    "        min_max = MinMaxScaler()\n",
    "        rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "        recommend_vector = user_interactions*rec_vector_scaled\n",
    "\n",
    "        # Get all the artist indices in order of recommendations (descending) and\n",
    "        # select only the top \"num_items\" items. \n",
    "        ranking = np.argsort(recommend_vector)[::-1]\n",
    "\n",
    "        ranking = self._filter_seen(playlist_id, ranking)\n",
    "\n",
    "        return ranking[:at]\n",
    "    \n",
    "    def _filter_seen(self, playlist_id, ranking):\n",
    "        user_profile = self.Cui[playlist_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]\n",
    "\n",
    "    def fit(self, Cui, features=10, iterations=20, lambda_val=0.1):\n",
    "        user_size, item_size = Cui.shape\n",
    "\n",
    "        X = np.random.rand(user_size, features) * 0.01\n",
    "        Y = np.random.rand(item_size, features) * 0.01\n",
    "\n",
    "        self.Cui, self.Ciu = Cui, Cui.T\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            print ('iteration %d of %d' % (iteration+1, iterations))\n",
    "            self.least_squares_cg(self.Cui, X, Y, lambda_val)\n",
    "            self.least_squares_cg(self.Ciu, Y, X, lambda_val)\n",
    "\n",
    "        self.X_sparse = sps.csr_matrix(X)\n",
    "        self.Y_sparse = sps.csr_matrix(Y)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 20\n",
      "iteration 2 of 20\n",
      "iteration 3 of 20\n",
      "iteration 4 of 20\n",
      "iteration 5 of 20\n",
      "iteration 6 of 20\n",
      "iteration 7 of 20\n",
      "iteration 8 of 20\n",
      "iteration 9 of 20\n",
      "iteration 10 of 20\n",
      "iteration 11 of 20\n",
      "iteration 12 of 20\n",
      "iteration 13 of 20\n",
      "iteration 14 of 20\n",
      "iteration 15 of 20\n",
      "iteration 16 of 20\n",
      "iteration 17 of 20\n",
      "iteration 18 of 20\n",
      "iteration 19 of 20\n",
      "iteration 20 of 20\n"
     ]
    }
   ],
   "source": [
    "alpha_val = 15\n",
    "conf_data = (URM_train * alpha_val).astype('double')\n",
    "rec_collab = collaborative_filtering()\n",
    "rec_collab.fit(conf_data, features=10, iterations=20, lambda_val=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_algorithm(URM_test, rec_collab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>it sucks</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Recommender'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39b5391c3b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mRecommender\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecommender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Recommender'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from Recommender import Recommender\n",
    "import subprocess\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MF_BPR_Cython(Recommender):\n",
    "\n",
    "\n",
    "    def __init__(self, URM_train, positive_threshold=4, recompile_cython = False,\n",
    "                 num_factors=10):\n",
    "\n",
    "\n",
    "        super(MF_BPR_Cython, self).__init__()\n",
    "\n",
    "\n",
    "        self.URM_train = URM_train\n",
    "        self.n_users = URM_train.shape[0]\n",
    "        self.n_items = URM_train.shape[1]\n",
    "        self.normalize = False\n",
    "        self.num_factors = num_factors\n",
    "        self.positive_threshold = positive_threshold\n",
    "\n",
    "        if recompile_cython:\n",
    "            print(\"Compiling in Cython\")\n",
    "            self.runCompilationScript()\n",
    "            print(\"Compilation Complete\")\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, epochs=30, logFile=None, URM_test=None, filterTopPop = False, filterCustomItems = np.array([], dtype=np.int), minRatingsPerUser=1,\n",
    "            batch_size = 1000, validate_every_N_epochs = 1, start_validation_after_N_epochs = 0,\n",
    "            learning_rate = 0.05, sgd_mode='sgd', user_reg = 0.0, positive_reg = 0.0, negative_reg = 0.0):\n",
    "\n",
    "\n",
    "        self.eligibleUsers = []\n",
    "\n",
    "        # Select only positive interactions\n",
    "        URM_train_positive = self.URM_train.copy()\n",
    "\n",
    "        URM_train_positive.data = URM_train_positive.data >= self.positive_threshold\n",
    "        URM_train_positive.eliminate_zeros()\n",
    "\n",
    "\n",
    "        for user_id in range(self.n_users):\n",
    "\n",
    "            start_pos = URM_train_positive.indptr[user_id]\n",
    "            end_pos = URM_train_positive.indptr[user_id+1]\n",
    "\n",
    "            numUserInteractions = len(URM_train_positive.indices[start_pos:end_pos])\n",
    "\n",
    "            if  numUserInteractions > 0 and numUserInteractions<self.n_items:\n",
    "                self.eligibleUsers.append(user_id)\n",
    "\n",
    "        # self.eligibleUsers contains the userID having at least one positive interaction and one item non observed\n",
    "        self.eligibleUsers = np.array(self.eligibleUsers, dtype=np.int64)\n",
    "        self.sgd_mode = sgd_mode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Import compiled module\n",
    "        from MatrixFactorization.Cython.MF_BPR_Cython_Epoch import MF_BPR_Cython_Epoch\n",
    "\n",
    "\n",
    "        self.cythonEpoch = MF_BPR_Cython_Epoch(URM_train_positive,\n",
    "                                                 self.eligibleUsers,\n",
    "                                                 num_factors = self.num_factors,\n",
    "                                                 learning_rate=learning_rate,\n",
    "                                                 batch_size=1,\n",
    "                                                 sgd_mode = sgd_mode,\n",
    "                                                 user_reg=user_reg,\n",
    "                                                 positive_reg=positive_reg,\n",
    "                                                 negative_reg=negative_reg)\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        start_time_train = time.time()\n",
    "\n",
    "        for currentEpoch in range(epochs):\n",
    "\n",
    "            start_time_epoch = time.time()\n",
    "\n",
    "            if currentEpoch > 0:\n",
    "                if self.batch_size>0:\n",
    "                    self.epochIteration()\n",
    "                else:\n",
    "                    print(\"No batch not available\")\n",
    "\n",
    "\n",
    "            if (URM_test is not None) and (currentEpoch % validate_every_N_epochs == 0) and \\\n",
    "                            currentEpoch >= start_validation_after_N_epochs:\n",
    "\n",
    "                print(\"Evaluation begins\")\n",
    "\n",
    "                self.W = self.cythonEpoch.get_W()\n",
    "                self.H = self.cythonEpoch.get_H()\n",
    "\n",
    "                results_run = self.evaluateRecommendations(URM_test,\n",
    "                                                           minRatingsPerUser=minRatingsPerUser)\n",
    "\n",
    "                self.writeCurrentConfig(currentEpoch, results_run, logFile)\n",
    "\n",
    "                print(\"Epoch {} of {} complete in {:.2f} minutes\".format(currentEpoch, epochs,\n",
    "                                                                     float(time.time() - start_time_epoch) / 60))\n",
    "\n",
    "\n",
    "            # Fit with no validation\n",
    "            else:\n",
    "                print(\"Epoch {} of {} complete in {:.2f} minutes\".format(currentEpoch, epochs,\n",
    "                                                                         float(time.time() - start_time_epoch) / 60))\n",
    "\n",
    "        # Ensure W and H are up to date\n",
    "        self.W = self.cythonEpoch.get_W()\n",
    "        self.H = self.cythonEpoch.get_H()\n",
    "\n",
    "        print(\"Fit completed in {:.2f} minutes\".format(float(time.time() - start_time_train) / 60))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def runCompilationScript(self):\n",
    "\n",
    "        # Run compile script setting the working directory to ensure the compiled file are contained in the\n",
    "        # appropriate subfolder and not the project root\n",
    "\n",
    "        compiledModuleSubfolder = \"/MatrixFactorization/Cython\"\n",
    "        fileToCompile_list = ['MF_BPR_Cython_Epoch.pyx']\n",
    "\n",
    "        for fileToCompile in fileToCompile_list:\n",
    "\n",
    "            command = ['python',\n",
    "                       'compileCython.py',\n",
    "                       fileToCompile,\n",
    "                       'build_ext',\n",
    "                       '--inplace'\n",
    "                       ]\n",
    "\n",
    "\n",
    "            output = subprocess.check_output(' '.join(command), shell=True, cwd=os.getcwd() + compiledModuleSubfolder)\n",
    "\n",
    "            try:\n",
    "\n",
    "                command = ['cython',\n",
    "                           fileToCompile,\n",
    "                           '-a'\n",
    "                           ]\n",
    "\n",
    "                output = subprocess.check_output(' '.join(command), shell=True, cwd=os.getcwd() + compiledModuleSubfolder)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        print(\"Compiled module saved in subfolder: {}\".format(compiledModuleSubfolder))\n",
    "\n",
    "        # Command to run compilation script\n",
    "        #python compileCython.py MF_BPR_Cython_Epoch.pyx build_ext --inplace\n",
    "\n",
    "        # Command to generate html report\n",
    "        #subprocess.call([\"cython\", \"-a\", \"MF_BPR_Cython_Epoch.pyx\"])\n",
    "\n",
    "\n",
    "    def epochIteration(self):\n",
    "\n",
    "        self.cythonEpoch.epochIteration_Cython()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def writeCurrentConfig(self, currentEpoch, results_run, logFile):\n",
    "\n",
    "        current_config = {'learn_rate': self.learning_rate,\n",
    "                          'num_factors': self.num_factors,\n",
    "                          'batch_size': 1,\n",
    "                          'epoch': currentEpoch}\n",
    "\n",
    "        print(\"Test case: {}\\nResults {}\\n\".format(current_config, results_run))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if (logFile != None):\n",
    "            logFile.write(\"Test case: {}, Results {}\\n\".format(current_config, results_run))\n",
    "            logFile.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def recommend(self, user_id, n=None, exclude_seen=True, filterTopPop = False, filterCustomItems = False):\n",
    "\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM_train[user_id]\n",
    "\n",
    "        scores_array = np.dot(self.W[user_id], self.H.T)\n",
    "\n",
    "\n",
    "        if self.normalize:\n",
    "            # normalization will keep the scores in the same range\n",
    "            # of value of the ratings in dataset\n",
    "            rated = user_profile.copy()\n",
    "            rated.data = np.ones_like(rated.data)\n",
    "            if self.sparse_weights:\n",
    "                den = rated.dot(self.W_sparse).toarray().ravel()\n",
    "            else:\n",
    "                den = rated.dot(self.W).ravel()\n",
    "            den[np.abs(den) < 1e-6] = 1.0  # to avoid NaNs\n",
    "            scores_array /= den\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores_array = self._filter_seen_on_scores(user_id, scores_array)\n",
    "\n",
    "        if filterTopPop:\n",
    "            scores_array = self._filter_TopPop_on_scores(scores_array)\n",
    "\n",
    "        if filterCustomItems:\n",
    "            scores_array = self._filterCustomItems_on_scores(scores_array)\n",
    "\n",
    "\n",
    "        # rank items and mirror column to obtain a ranking in descending score\n",
    "        #ranking = scores.argsort()\n",
    "        #ranking = np.flip(ranking, axis=0)\n",
    "\n",
    "        # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "        # - Partition the data to extract the set of relevant items\n",
    "        # - Sort only the relevant items\n",
    "        # - Get the original item index\n",
    "        relevant_items_partition = (-scores_array).argpartition(n)[0:n]\n",
    "        relevant_items_partition_sorting = np.argsort(-scores_array[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "\n",
    "        return ranking\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And the cython code for the update\n",
    "\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "#cython: initializedcheck=False\n",
    "#cython: language_level=3\n",
    "#cython: nonecheck=False\n",
    "#cython: cdivision=True\n",
    "#cython: unpack_method_calls=True\n",
    "#cython: overflowcheck=False\n",
    "\n",
    "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
    "\n",
    "from Recommender_utils import check_matrix\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from libc.math cimport exp, sqrt\n",
    "from libc.stdlib cimport rand, RAND_MAX\n",
    "\n",
    "\n",
    "cdef struct BPR_sample:\n",
    "    long user\n",
    "    long pos_item\n",
    "    long neg_item\n",
    "\n",
    "\n",
    "cdef class MF_BPR_Cython_Epoch:\n",
    "\n",
    "    cdef int n_users\n",
    "    cdef int n_items, num_factors\n",
    "    cdef int numPositiveIteractions\n",
    "\n",
    "    cdef int useAdaGrad, rmsprop\n",
    "\n",
    "    cdef float learning_rate, user_reg, positive_reg, negative_reg\n",
    "\n",
    "    cdef int batch_size, sparse_weights\n",
    "\n",
    "    cdef long[:] eligibleUsers\n",
    "    cdef long numEligibleUsers\n",
    "\n",
    "    cdef int[:] seenItemsSampledUser\n",
    "    cdef int numSeenItemsSampledUser\n",
    "\n",
    "    cdef int[:] URM_mask_indices, URM_mask_indptr\n",
    "\n",
    "    cdef double[:,:] W, H\n",
    "\n",
    "\n",
    "    def __init__(self, URM_mask, eligibleUsers, num_factors,\n",
    "                 learning_rate = 0.05, user_reg = 0.0, positive_reg = 0.0, negative_reg = 0.0,\n",
    "                 batch_size = 1, sgd_mode='sgd'):\n",
    "\n",
    "        super(MF_BPR_Cython_Epoch, self).__init__()\n",
    "\n",
    "\n",
    "        URM_mask = check_matrix(URM_mask, 'csr')\n",
    "\n",
    "        self.numPositiveIteractions = int(URM_mask.nnz * 1)\n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "        self.num_factors = num_factors\n",
    "\n",
    "        self.URM_mask_indices = URM_mask.indices\n",
    "        self.URM_mask_indptr = URM_mask.indptr\n",
    "\n",
    "        # W and H cannot be initialized as zero, otherwise the gradient will always be zero\n",
    "        self.W = np.random.random((self.n_users, self.num_factors))\n",
    "        self.H = np.random.random((self.n_items, self.num_factors))\n",
    "\n",
    "\n",
    "\n",
    "        if sgd_mode=='sgd':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"SGD_mode not valid. Acceptable values are: 'sgd'. Provided value was '{}'\".format(\n",
    "                    sgd_mode))\n",
    "\n",
    "\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.user_reg = user_reg\n",
    "        self.positive_reg = positive_reg\n",
    "        self.negative_reg = negative_reg\n",
    "\n",
    "\n",
    "        if batch_size!=1:\n",
    "            print(\"MiniBatch not implemented, reverting to default value 1\")\n",
    "        self.batch_size = 1\n",
    "\n",
    "        self.eligibleUsers = eligibleUsers\n",
    "        self.numEligibleUsers = len(eligibleUsers)\n",
    "\n",
    "\n",
    "    # Using memoryview instead of the sparse matrix itself allows for much faster access\n",
    "    cdef int[:] getSeenItems(self, long index):\n",
    "        return self.URM_mask_indices[self.URM_mask_indptr[index]:self.URM_mask_indptr[index + 1]]\n",
    "\n",
    "\n",
    "\n",
    "    def epochIteration_Cython(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        cdef long totalNumberOfBatch = int(self.numPositiveIteractions / self.batch_size) + 1\n",
    "\n",
    "\n",
    "        cdef BPR_sample sample\n",
    "        cdef long u, i, j\n",
    "        cdef long index, numCurrentBatch\n",
    "        cdef double x_uij, sigmoid\n",
    "\n",
    "        cdef int numSeenItems\n",
    "\n",
    "        # Variables for AdaGrad and RMSprop\n",
    "        cdef double [:] sgd_cache\n",
    "        cdef double cacheUpdate\n",
    "        cdef float gamma\n",
    "\n",
    "        cdef double H_i, H_j, W_u\n",
    "\n",
    "        #\n",
    "        # if self.useAdaGrad:\n",
    "        #     sgd_cache = np.zeros((self.n_items), dtype=float)\n",
    "        #\n",
    "        # elif self.rmsprop:\n",
    "        #     sgd_cache = np.zeros((self.n_items), dtype=float)\n",
    "        #     gamma = 0.001\n",
    "\n",
    "\n",
    "        cdef long start_time_epoch = time.time()\n",
    "        cdef long start_time_batch = time.time()\n",
    "\n",
    "        for numCurrentBatch in range(totalNumberOfBatch):\n",
    "\n",
    "            # Uniform user sampling with replacement\n",
    "            sample = self.sampleBatch_Cython()\n",
    "\n",
    "            u = sample.user\n",
    "            i = sample.pos_item\n",
    "            j = sample.neg_item\n",
    "\n",
    "            x_uij = 0.0\n",
    "\n",
    "            for index in range(self.num_factors):\n",
    "\n",
    "                x_uij += self.W[u,index] * (self.H[i,index] - self.H[j,index])\n",
    "\n",
    "            # Use gradient of log(sigm(-x_uij))\n",
    "            sigmoid = 1 / (1 + exp(x_uij))\n",
    "\n",
    "\n",
    "            #   OLD CODE, YOU MAY TRY TO USE IT\n",
    "            #\n",
    "            # if self.useAdaGrad:\n",
    "            #     cacheUpdate = gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] += cacheUpdate\n",
    "            #     sgd_cache[j] += cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "            #\n",
    "            # elif self.rmsprop:\n",
    "            #     cacheUpdate = sgd_cache[i] * gamma + (1 - gamma) * gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] += cacheUpdate\n",
    "            #     sgd_cache[j] += cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "\n",
    "\n",
    "            for index in range(self.num_factors):\n",
    "\n",
    "                # Copy original value to avoid messing up the updates\n",
    "                H_i = self.H[i, index]\n",
    "                H_j = self.H[j, index]\n",
    "                W_u = self.W[u, index]\n",
    "\n",
    "                self.W[u, index] += self.learning_rate * (sigmoid * ( H_i - H_j ) - self.user_reg * W_u)\n",
    "                self.H[i, index] += self.learning_rate * (sigmoid * ( W_u ) - self.positive_reg * H_i)\n",
    "                self.H[j, index] += self.learning_rate * (sigmoid * (-W_u ) - self.negative_reg * H_j)\n",
    "\n",
    "\n",
    "\n",
    "            if((numCurrentBatch%5000000==0 and not numCurrentBatch==0) or numCurrentBatch==totalNumberOfBatch-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                    numCurrentBatch*self.batch_size,\n",
    "                    100.0* float(numCurrentBatch*self.batch_size)/self.numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    float(numCurrentBatch*self.batch_size + 1) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "\n",
    "    def get_W(self):\n",
    "\n",
    "        return np.array(self.W)\n",
    "\n",
    "\n",
    "    def get_H(self):\n",
    "        return np.array(self.H)\n",
    "\n",
    "\n",
    "\n",
    "    cdef BPR_sample sampleBatch_Cython(self):\n",
    "\n",
    "        cdef BPR_sample sample = BPR_sample()\n",
    "        cdef long index\n",
    "        cdef int negItemSelected\n",
    "\n",
    "        # Warning: rand() returns an integer\n",
    "\n",
    "        index = rand() % self.numEligibleUsers\n",
    "\n",
    "        sample.user = self.eligibleUsers[index]\n",
    "\n",
    "        self.seenItemsSampledUser = self.getSeenItems(sample.user)\n",
    "        self.numSeenItemsSampledUser = len(self.seenItemsSampledUser)\n",
    "\n",
    "        index = rand() % self.numSeenItemsSampledUser\n",
    "\n",
    "        sample.pos_item = self.seenItemsSampledUser[index]\n",
    "\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        # for every user\n",
    "        while (not negItemSelected):\n",
    "            sample.neg_item = rand() % self.n_items\n",
    "\n",
    "            index = 0\n",
    "            while index < self.numSeenItemsSampledUser and self.seenItemsSampledUser[index]!=sample.neg_item:\n",
    "                index+=1\n",
    "\n",
    "            if index == self.numSeenItemsSampledUser:\n",
    "                negItemSelected = True\n",
    "\n",
    "return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Movielens10MReader import Movielens10MReader\n",
    "\n",
    "dataReader = Movielens10MReader()\n",
    "\n",
    "URM_train = dataReader.get_URM_train()\n",
    "URM_test = dataReader.get_URM_test()\n",
    "\n",
    "recommender = MF_BPR_Cython(URM_train, recompile_cython=False, positive_threshold=4)\n",
    "\n",
    "logFile = open(\"Result_log.txt\", \"a\")\n",
    "\n",
    "recommender.fit(epochs=5, validate_every_N_epochs=2, URM_test=URM_test,\n",
    "                logFile=logFile, batch_size=1, sgd_mode='sgd', learning_rate=1e-4)\n",
    "\n",
    "#results_run = recommender.evaluateRecommendations(URM_test, at=5)\n",
    "#print(results_run)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RecSysChallenge_AimiCollini.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
